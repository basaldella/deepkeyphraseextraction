An Evaluation of Statistical Approaches to Text Categorization.
This paper focuses on a comparative evaluation of a wide-range of

text categorization methods, including previously published results on the

Reuters corpus and new results of additional experiments. A

controlled study using three classifiers, kNN, LLSF and WORD, was

conducted to examine the impact of configuration variations in five

versions of Reuters on the observed performance of classifiers.

Analysis and empirical evidence suggest that the evaluation results on

some versions of Reuters were significantly affected by the inclusion

of a large portion of unlabelled documents, mading those results

difficult to interpret and leading to considerable confusions in the

literature. Using the results evaluated on the other versions of

Reuters which exclude the unlabelled documents, the performance of

twelve methods are compared directly or indirectly. For indirect

compararions, kNN, LLSF and WORD were used as baselines, since they

were evaluated on all versions of Reuters that exclude the unlabelled

documents. As a global observation, kNN, LLSF and a neural network

method had the best performance&semi; except for a Naive Bayes approach,

the other learning algorithms also performed relatively well.
